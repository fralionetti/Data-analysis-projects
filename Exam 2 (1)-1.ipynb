{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e220f37",
   "metadata": {},
   "source": [
    "# Introductory steps: loding all relevant packages, loading the dataset and cleaning it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2935a0",
   "metadata": {},
   "source": [
    "First, I install and load all of the relevant packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e7c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: embeddingvectorizer in c:\\users\\franc\\onedrive\\documents\\uva master\\phyton\\lib\\site-packages (0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\franc\\onedrive\\documents\\uva master\\phyton\\lib\\site-packages (from embeddingvectorizer) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\franc\\onedrive\\documents\\uva master\\phyton\\lib\\site-packages (from scikit-learn->embeddingvectorizer) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\franc\\onedrive\\documents\\uva master\\phyton\\lib\\site-packages (from scikit-learn->embeddingvectorizer) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\franc\\onedrive\\documents\\uva master\\phyton\\lib\\site-packages (from scikit-learn->embeddingvectorizer) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\franc\\onedrive\\documents\\uva master\\phyton\\lib\\site-packages (from scikit-learn->embeddingvectorizer) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install embeddingvectorizer\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import re\n",
    "import joblib\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load(\"W2V.kv\", mmap='r+')\n",
    "vocabs = word_vectors.index_to_key\n",
    "vectors = word_vectors.vectors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from embeddingvectorizer import EmbeddingCountVectorizer, EmbeddingTfidfVectorizer\n",
    "import embeddingvectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be575a2b",
   "metadata": {},
   "source": [
    "Then, I load the document and I have a quick look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e9095d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>blurb</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ora parla l’economista francese e avverte Ue: ...</td>\n",
       "      <td>La Commissione europea, con l’Italia, sta gioc...</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FB chiede alle banche i dati dei clienti - Int...</td>\n",
       "      <td>Facebook ha chiesto alle maggiori banche ameri...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Poste Italiane: partono le Domande, in tutta I...</td>\n",
       "      <td>Affinché i Portalettere in servizio possano an...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aspirina pericolosa per i malatti di cuore, ri...</td>\n",
       "      <td>Il ministero della Salute Britannico ha invita...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Il San Carlo a Milano, Mehta: «Spero ci siano ...</td>\n",
       "      <td>L’orchestra e il coro del Massimo in concerto ...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ora parla l’economista francese e avverte Ue: ...   \n",
       "1  FB chiede alle banche i dati dei clienti - Int...   \n",
       "2  Poste Italiane: partono le Domande, in tutta I...   \n",
       "3  Aspirina pericolosa per i malatti di cuore, ri...   \n",
       "4  Il San Carlo a Milano, Mehta: «Spero ci siano ...   \n",
       "\n",
       "                                               blurb     topic  \n",
       "0  La Commissione europea, con l’Italia, sta gioc...   Economy  \n",
       "1  Facebook ha chiesto alle maggiori banche ameri...     Other  \n",
       "2  Affinché i Portalettere in servizio possano an...     Other  \n",
       "3  Il ministero della Salute Britannico ha invita...  Politics  \n",
       "4  L’orchestra e il coro del Massimo in concerto ...  Politics  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"labeled.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f7e4e",
   "metadata": {},
   "source": [
    "First, I check whether the text contains E-mail addresses, HTML tags and HTML Character escapes and I remove them. I do not drop the numbers and the stopwords as that will be done automatically with my tokenizer. Then, I check that all of the characters were actually removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e53388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.topic.str.contains(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0421fca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.topic.str.contains(r\"</?\\w[^>]*>\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ec94cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.topic.str.contains(r\"&[^;]+;\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88afdab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title.str.contains(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f70cbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title.str.contains(r\"</?\\w[^>]*>\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2432db97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title.str.contains(r\"&[^;]+;\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f210c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.blurb.str.contains(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28dde1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.blurb.str.contains(r\"</?\\w[^>]*>\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e5097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.blurb.str.contains(r\"&[^;]+;\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "375c60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'] = data['title'].str.replace(r\"&[^;]+;\", '', regex=True)\n",
    "data['title'] = data['title'].str.replace(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\", '', regex=True)\n",
    "data['blurb'] = data['blurb'].str.replace(r\"&[^;]+;\", '', regex=True)\n",
    "data['blurb'] = data['blurb'].str.replace(r\"</?\\w[^>]*>\", '', regex=True)\n",
    "data['blurb'] = data['blurb'].str.replace(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\", '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b642730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title.str.contains(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40476c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title.str.contains(r\"&[^;]+;\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2060e2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.blurb.str.contains(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc4e6a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.blurb.str.contains(r\"</?\\w[^>]*>\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54476575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.blurb.str.contains(r\"&[^;]+;\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ec311",
   "metadata": {},
   "source": [
    "Now I check whether there are NAs. I see that there are 11. Therefore, I check where they are contained and I remove them. Then, I check that everything went well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e93ab009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65dc7ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebeab242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['blurb'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "126b9df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['topic'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68bc8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['blurb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ab19ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ef8aa",
   "metadata": {},
   "source": [
    "Now, I join the columns blurb and text together. I do so, so that the classifier will be trained only on one column. Instead of keeping only one of them, I combine them together so that the classifier will have more text to be trained on. Then, I remove the columns blurb and title as they are not important anymore. I also check what values the column topic containes and then I change all of the values that are different from politics to other, as I want the classifier to distinguish only political news from all other types of news (therefore, the types of the other news are not relevant anymore). I then check that everything went well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d436284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Economy', 'Other', 'Politics', 'CrimeDisaster', 'Entertainment',\n",
       "       'Culture', 'ScienceTech', 'Sports'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['title'] + ' ' + data['blurb']\n",
    "data = data.drop(columns=[\"title\", \"blurb\"])\n",
    "data[\"topic\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8722f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Other', 'Politics'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['topic'] = data['topic'].replace(['Economy', 'CrimeDisaster', 'Entertainment', 'Culture', 'ScienceTech', 'Sports'], 'Other')\n",
    "data[\"topic\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d97457d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other</td>\n",
       "      <td>Ora parla l’economista francese e avverte Ue: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other</td>\n",
       "      <td>FB chiede alle banche i dati dei clienti - Int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Other</td>\n",
       "      <td>Poste Italiane: partono le Domande, in tutta I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Aspirina pericolosa per i malatti di cuore, ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Il San Carlo a Milano, Mehta: «Spero ci siano ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic                                               text\n",
       "0     Other  Ora parla l’economista francese e avverte Ue: ...\n",
       "1     Other  FB chiede alle banche i dati dei clienti - Int...\n",
       "2     Other  Poste Italiane: partono le Domande, in tutta I...\n",
       "3  Politics  Aspirina pericolosa per i malatti di cuore, ri...\n",
       "4  Politics  Il San Carlo a Milano, Mehta: «Spero ci siano ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec0dc0",
   "metadata": {},
   "source": [
    "Then, I create my tokenizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27385d1f",
   "metadata": {},
   "source": [
    "# Creating the tokenizer and splitting the datset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fbcc67",
   "metadata": {},
   "source": [
    "I create my tokenizer and choose Italian as a language as the texts are in Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8f859d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def tokenize(self, text):\n",
    "        result = []\n",
    "        word =  r\"\\p{letter}\"\n",
    "        tokens = nltk.word_tokenize(text, language = \"Italian\")\n",
    "        tokens = [t for t in tokens if regex.search(word, t)]\n",
    "        result += tokens\n",
    "        return result\n",
    "    \n",
    "mytokenizer = MyTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816bea3",
   "metadata": {},
   "source": [
    "Now, I divide the data into two subsets: a train a and a test subset. The test subset is set aside to test the classifier after it has been traied on the train subset (this will be done later). I check how the tokenizer perform with 2 sentences taken from the data. It performs quite well: punctuation and numebrs are removed, articles and words are identified well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d51a6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2056,) (2056,)\n",
      "(514,) (514,)\n",
      "['L', 'incidente', 'nel', 'pomeriggio', 'a', 'Caramanico', 'Terme', 'in', 'localit', 'agrave', 'San', 'Tommaso', 'Pescara', 'Le', 'vittime', 'una', 'coppia', 'erano', 'in', 'gita', 'con', 'i', 'figli', 'di', 'e', 'anni', 'nbsp', 'e', 'alcuni', 'amici', 'Buio', 'e', 'zona', 'impervia', 'rendono', 'difficile', 'il', 'recupero', 'delle', 'salme']\n",
      "['La', 'Commissione', 'europea', 'con', 'lâ€™Italia', 'sta', 'giocando', 'qout', 'un', 'gioco', 'pericoloso', 'qout', 'A', 'dirlo', 'Ã¨', 'Steve', 'Ohana', 'economista', 'francese', 'della', 'Escp', 'School', 'qout', 'Non', 'reagire', 'alla', 'manovra', 'italiana', 'avrebbe', 'significato', 'una', 'grossa', 'perdita', 'di', 'credibilitÃ', 'per', 'la', 'Commissione', 'e', 'piÃ¹', 'in', 'generale', 'per', 'le', 'istituzio']\n",
      "['Brno', 'doppietta', 'Ducati', 'Dovizioso', 'piega', 'Lorenzo', 'Marquez', 'poi', 'Rossi', 'Splendida', 'vittoria', 'del', 'forlivese', 'che', 'regge', 'agli', 'assalti', 'dei', 'due', 'spagnoli', 'e', 'conquista', 'il', 'secondo', 'successo', 'stagionale', 'dopo', 'un', 'intenso', 'duello', 'Vale', 'chiude', 'ai', 'piedi', 'del', 'podio']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"topic\"], test_size=0.2, random_state=5)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "sentence = \"La Commissione europea, con lâ€™Italia, sta giocando &qout;un gioco pericoloso&qout;. A dirlo Ã¨ Steve Ohana, economista francese della Escp School. &qout;Non reagire alla manovra italiana avrebbe significato una grossa perdita di credibilitÃ  per la Commissione e, piÃ¹ in generale, per le istituzio...\"\n",
    "sentence2 = \"L&#39;incidente nel pomeriggio a Caramanico Terme, in localit&agrave; San Tommaso (Pescara). Le vittime, una coppia, erano in gita con i figli di 5 e 8 anni&nbsp; e alcuni amici. Buio e zona impervia rendono difficile il recupero delle salme\"\n",
    "sentence3 = \"Brno, doppietta Ducati: Dovizioso piega Lorenzo. 3° Marquez, poi Rossi Splendida vittoria del forlivese che regge agli assalti dei due spagnoli e conquista il secondo successo stagionale dopo un intenso duello. Vale chiude ai piedi del podio\"\n",
    "print(mytokenizer.tokenize(sentence2))\n",
    "print(mytokenizer.tokenize(sentence))\n",
    "print(mytokenizer.tokenize(sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa619e",
   "metadata": {},
   "source": [
    "Then, I didive the train split that I just created again. I will use this new train/validation split to analyse the various vectorizers and classifiers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d465619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1644,) (1644,)\n",
      "(412,) (412,)\n"
     ]
    }
   ],
   "source": [
    "X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)\n",
    "print(X_train_new.shape, y_train_new.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7859d",
   "metadata": {},
   "source": [
    "# Testing the models with BOW approach and beyond BOW approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320649ea",
   "metadata": {},
   "source": [
    "Now, I check how various models work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccfd2fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\OneDrive\\Documents\\uVa Master\\phyton\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7766990291262136\n",
      "\n",
      "\n",
      "NB-TfIdf\n",
      "0.7742718446601942\n",
      "\n",
      "\n",
      "LR-Count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\OneDrive\\Documents\\uVa Master\\phyton\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7597087378640777\n",
      "\n",
      "\n",
      "LR-TfIdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\OneDrive\\Documents\\uVa Master\\phyton\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669902912621359\n",
      "\n",
      "\n",
      "SVC-count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\OneDrive\\Documents\\uVa Master\\phyton\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7475728155339806\n",
      "\n",
      "\n",
      "SVC-TfIdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\OneDrive\\Documents\\uVa Master\\phyton\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7694174757281553\n",
      "\n",
      "\n",
      "RF-Count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\OneDrive\\Documents\\uVa Master\\phyton\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7597087378640777\n",
      "\n",
      "\n",
      "RF-TfIdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\OneDrive\\Documents\\uVa Master\\phyton\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.779126213592233\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    (\"NB-count\", CountVectorizer(tokenizer=mytokenizer.tokenize, min_df=5, max_df=.5), MultinomialNB()),\n",
    "    (\"NB-TfIdf\", TfidfVectorizer(min_df=5, max_df=.5), MultinomialNB()),\n",
    "    (\n",
    "        \"LR-Count\", CountVectorizer(tokenizer=mytokenizer.tokenize, min_df=5, max_df=.5),\n",
    "        LogisticRegression(solver=\"liblinear\"),\n",
    "    ),\n",
    "    (\n",
    "        \"LR-TfIdf\", TfidfVectorizer(tokenizer=mytokenizer.tokenize, min_df=5, max_df=.5),\n",
    "        LogisticRegression(solver=\"liblinear\"),\n",
    "    ),\n",
    "    (\n",
    "        \"SVC-count\", CountVectorizer(tokenizer=mytokenizer.tokenize, min_df=5, max_df=.5),\n",
    "        SVC(gamma=\"scale\"),\n",
    "    ),\n",
    "    (   \"SVC-TfIdf\", TfidfVectorizer(tokenizer=mytokenizer.tokenize, min_df=5, max_df=.5),\n",
    "        SVC(gamma=\"scale\"),\n",
    "    ),\n",
    "    (\n",
    "        \"RF-Count\", CountVectorizer(tokenizer=mytokenizer.tokenize, min_df=5, max_df=.5),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "    ),\n",
    "    (   \"RF-TfIdf\", TfidfVectorizer(tokenizer=mytokenizer.tokenize, min_df=5, max_df=.5),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for name, vectorizer, classifier in models:\n",
    "    print(name)\n",
    "    X_train2 = vectorizer.fit_transform(X_train_new)\n",
    "    X_test2 = vectorizer.transform(X_val)\n",
    "    classifier.fit(X_train2, y_train_new)\n",
    "    y_pred = classifier.predict(X_test2)\n",
    "    print(metrics.accuracy_score(y_val, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c5aee",
   "metadata": {},
   "source": [
    "I see that none of the values are above .8, which is what we should strive for. Therefore, I use a non-BOW approach to see if I can achieve better accuracy scores. I decided to use word embeddings because I think that they are quite useful in analysing Italian texts and news healdlines dealing with politics. Word embeddings are a powerful tool for natural language processing, as they allow you to represent words in a high-dimensional space, where words with similar meanings are located close to each other. This moves beyong the BOW approaches that takes into account simply the frequency of words. With word embeddings the classifier can be trained to find synonyms and analogies. Hence, this can help improve the accuracy of machine learning models by providing them with more informative features. I especially think that they are useful to analyse Italian, as it is a language rich of synonyms, analogies and comparisons. Differently form english, which is a quite straightforward and 'simplistic' language, Italian is a more nuanced and rich language. First, I create a dataframe with the words and their asisgned values and then I create a dict which I will then use to test various combiantions of vectorizers and classifiers with word embeddings. I try the two most common vectorizers (Count and Tfidf) and the main models that we studied so far. Because Naive Bayes cannot handle negative numbers, I do not include it in the mdoels tested. First, I check that the word embeddings work well. Moreover, I include random forests and not decision trees as, overall, the former perform better than the latter ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91e33404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>amphibiaweb.org</th>\n",
       "      <td>2.383386</td>\n",
       "      <td>1.243215</td>\n",
       "      <td>0.782704</td>\n",
       "      <td>-0.691165</td>\n",
       "      <td>-0.080835</td>\n",
       "      <td>-0.638561</td>\n",
       "      <td>0.840422</td>\n",
       "      <td>-0.200750</td>\n",
       "      <td>-0.346297</td>\n",
       "      <td>-0.177310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180366</td>\n",
       "      <td>-0.216721</td>\n",
       "      <td>-0.936201</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>0.211580</td>\n",
       "      <td>0.984754</td>\n",
       "      <td>1.653805</td>\n",
       "      <td>0.114008</td>\n",
       "      <td>-0.441808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andegavorum</th>\n",
       "      <td>2.299073</td>\n",
       "      <td>-0.154413</td>\n",
       "      <td>0.084963</td>\n",
       "      <td>-1.317420</td>\n",
       "      <td>-0.928468</td>\n",
       "      <td>0.823194</td>\n",
       "      <td>0.533603</td>\n",
       "      <td>1.324872</td>\n",
       "      <td>0.344809</td>\n",
       "      <td>0.539048</td>\n",
       "      <td>...</td>\n",
       "      <td>1.051035</td>\n",
       "      <td>0.421230</td>\n",
       "      <td>-0.626271</td>\n",
       "      <td>0.624682</td>\n",
       "      <td>0.080760</td>\n",
       "      <td>0.849758</td>\n",
       "      <td>0.217712</td>\n",
       "      <td>0.712347</td>\n",
       "      <td>0.353163</td>\n",
       "      <td>0.887824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reiske</th>\n",
       "      <td>2.295041</td>\n",
       "      <td>0.204441</td>\n",
       "      <td>-0.022760</td>\n",
       "      <td>-0.315606</td>\n",
       "      <td>-0.029753</td>\n",
       "      <td>0.225191</td>\n",
       "      <td>0.215381</td>\n",
       "      <td>0.882625</td>\n",
       "      <td>0.238328</td>\n",
       "      <td>0.765809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305810</td>\n",
       "      <td>-0.304357</td>\n",
       "      <td>0.177891</td>\n",
       "      <td>0.143306</td>\n",
       "      <td>-0.139650</td>\n",
       "      <td>0.563671</td>\n",
       "      <td>-0.424972</td>\n",
       "      <td>-0.311885</td>\n",
       "      <td>-0.303003</td>\n",
       "      <td>0.353190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginem</th>\n",
       "      <td>2.273339</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>-0.029058</td>\n",
       "      <td>-0.793297</td>\n",
       "      <td>-0.021337</td>\n",
       "      <td>-0.441115</td>\n",
       "      <td>-0.172477</td>\n",
       "      <td>0.717569</td>\n",
       "      <td>-0.402575</td>\n",
       "      <td>-0.233796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220241</td>\n",
       "      <td>0.241052</td>\n",
       "      <td>0.109762</td>\n",
       "      <td>0.646236</td>\n",
       "      <td>0.351735</td>\n",
       "      <td>-0.240822</td>\n",
       "      <td>-0.089408</td>\n",
       "      <td>0.772591</td>\n",
       "      <td>-0.059612</td>\n",
       "      <td>-0.367220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bruys</th>\n",
       "      <td>2.262714</td>\n",
       "      <td>0.225114</td>\n",
       "      <td>0.016751</td>\n",
       "      <td>-0.411804</td>\n",
       "      <td>-0.056492</td>\n",
       "      <td>0.211376</td>\n",
       "      <td>0.192823</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>0.814759</td>\n",
       "      <td>-0.872253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257632</td>\n",
       "      <td>-0.106817</td>\n",
       "      <td>0.672275</td>\n",
       "      <td>-0.705625</td>\n",
       "      <td>1.161071</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>0.246170</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>0.059887</td>\n",
       "      <td>0.349734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1         2         3         4         5    \\\n",
       "amphibiaweb.org  2.383386  1.243215  0.782704 -0.691165 -0.080835 -0.638561   \n",
       "andegavorum      2.299073 -0.154413  0.084963 -1.317420 -0.928468  0.823194   \n",
       "reiske           2.295041  0.204441 -0.022760 -0.315606 -0.029753  0.225191   \n",
       "virginem         2.273339  0.794300 -0.029058 -0.793297 -0.021337 -0.441115   \n",
       "bruys            2.262714  0.225114  0.016751 -0.411804 -0.056492  0.211376   \n",
       "\n",
       "                      6         7         8         9    ...       290  \\\n",
       "amphibiaweb.org  0.840422 -0.200750 -0.346297 -0.177310  ...  0.180366   \n",
       "andegavorum      0.533603  1.324872  0.344809  0.539048  ...  1.051035   \n",
       "reiske           0.215381  0.882625  0.238328  0.765809  ... -0.305810   \n",
       "virginem        -0.172477  0.717569 -0.402575 -0.233796  ... -0.220241   \n",
       "bruys            0.192823  0.057007  0.814759 -0.872253  ... -0.257632   \n",
       "\n",
       "                      291       292       293       294       295       296  \\\n",
       "amphibiaweb.org -0.216721 -0.936201  0.130300 -0.004838  0.211580  0.984754   \n",
       "andegavorum      0.421230 -0.626271  0.624682  0.080760  0.849758  0.217712   \n",
       "reiske          -0.304357  0.177891  0.143306 -0.139650  0.563671 -0.424972   \n",
       "virginem         0.241052  0.109762  0.646236  0.351735 -0.240822 -0.089408   \n",
       "bruys           -0.106817  0.672275 -0.705625  1.161071  0.008391  0.246170   \n",
       "\n",
       "                      297       298       299  \n",
       "amphibiaweb.org  1.653805  0.114008 -0.441808  \n",
       "andegavorum      0.712347  0.353163  0.887824  \n",
       "reiske          -0.311885 -0.303003  0.353190  \n",
       "virginem         0.772591 -0.059612 -0.367220  \n",
       "bruys            0.354919  0.059887  0.349734  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wvdf = pd.DataFrame(word_vectors.vectors, index=word_vectors.index_to_key)\n",
    "word_vectors2_dict = dict(zip(vocabs, vectors))\n",
    "wvdf.sort_values(0, ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0a92f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR-Count\n",
      "0.7669902912621359\n",
      "\n",
      "\n",
      "LR-TfIdf\n",
      "0.8131067961165048\n",
      "\n",
      "\n",
      "SVC-count\n",
      "0.8155339805825242\n",
      "\n",
      "\n",
      "SVC-TfIdf\n",
      "0.8179611650485437\n",
      "\n",
      "\n",
      "RF-Count\n",
      "0.7936893203883495\n",
      "\n",
      "\n",
      "RF-TfIdf\n",
      "0.7766990291262136\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelsWE = [\n",
    "    (\n",
    "        \"LR-Count\",\n",
    "        embeddingvectorizer.EmbeddingCountVectorizer(word_vectors2_dict, operator='sum'),\n",
    "        LogisticRegression(solver=\"liblinear\"),\n",
    "    ),\n",
    "    (\n",
    "        \"LR-TfIdf\",\n",
    "        embeddingvectorizer.EmbeddingTfidfVectorizer(word_vectors2_dict, operator='sum'),\n",
    "        LogisticRegression(solver=\"liblinear\"),\n",
    "    ),\n",
    "    (\n",
    "        \"SVC-count\",\n",
    "        embeddingvectorizer.EmbeddingCountVectorizer(word_vectors2_dict, operator='sum'),\n",
    "        SVC(gamma=\"scale\"),\n",
    "    ),\n",
    "    (   \"SVC-TfIdf\",\n",
    "        embeddingvectorizer.EmbeddingTfidfVectorizer(word_vectors2_dict, operator='sum'),\n",
    "        SVC(gamma=\"scale\"),\n",
    "    ),\n",
    "    (\n",
    "        \"RF-Count\",\n",
    "        embeddingvectorizer.EmbeddingCountVectorizer(word_vectors2_dict, operator='sum'),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "    ),\n",
    "    (   \"RF-TfIdf\",\n",
    "        embeddingvectorizer.EmbeddingTfidfVectorizer(word_vectors2_dict, operator='sum'),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for name, vectorizer, classifier in modelsWE:\n",
    "    print(name)\n",
    "    X_train3 = np.array(list(vectorizer.fit_transform(X_train_new)))\n",
    "    X_test3 = np.array(list(vectorizer.transform(X_val)))\n",
    "    classifier.fit(X_train3, y_train_new)\n",
    "    y_pred = classifier.predict(X_test3)\n",
    "    print(metrics.accuracy_score(y_val, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403313a",
   "metadata": {},
   "source": [
    "# Tuning the models and choosing the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465a075",
   "metadata": {},
   "source": [
    "As expected, word embedding improve most the models' performance quite much. I see that the SVC-TfIdf and the SVC-count models are the one performing the best (they have the highest accuracy score) and, therefore, I will tune these one model. I will only look at parameters C as my computer could not handle any research with more parameters (such as kernel and grade) and the same occurred with collab. As value C controls the tradeoff between the complexity of the decision boundary and the amount of classification error that is permitted, testing different values of C is important because it allows us to find the best tradeoff between overfitting and underfitting the data, and thus, helps to obtain better generalization performance on unseen data. Then, I choose my best model and provide a final classification report generated on the test data set aside at the beginning. \n",
    "Unfortunately my laptop crashed multiple times when running the search grid (I think because it does not have enough memory). I was able to run it once, but not again. Therefore, I copy-pasted here the code that I used and the output that I got. \n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(word_vectors2_dict, operator='mean')),\n",
    "        (\"classifier\", SVC(gamma=\"scale\")),\n",
    "    ]\n",
    ")\n",
    "grid = {\n",
    "    \"classifier__C\": [0.01, 1, 100],\n",
    "}\n",
    "search = GridSearchCV(\n",
    "    estimator=pipeline, n_jobs=-1, param_grid=grid, scoring=\"accuracy\", cv=5\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best accuracy: {search.best_score_}\")\n",
    "\n",
    "Best parameters: {'classifier__C': 1}\n",
    "0.8131067961165048\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"vectorizer\", embeddingvectorizer.EmbeddingTfidfVectorizer(word_vectors2_dict, operator='mean')),\n",
    "        (\"classifier\", SVC(gamma=\"scale\")),\n",
    "    ]\n",
    ")\n",
    "grid = {\n",
    "    \"classifier__C\": [0.01, 1, 100],\n",
    "}\n",
    "search = GridSearchCV(\n",
    "    estimator=pipeline, n_jobs=-1, param_grid=grid, scoring=\"accuracy\", cv=5\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best accuracy: {search.best_score_}\")\n",
    "\n",
    "Best parameters: {'classifier__C': 1}\n",
    "Best accuracy: 0.8180922684430586\n",
    "\n",
    "\n",
    "As the SVC model witht he count vectorizer turns out to be slight more accurate, I choose this as my final model to test the test data that I set out at the beginning. Now, I don't look only at the accuracy score, but I also generate a classification report. This gives a better overview as it shows the precision, recall and F1 values for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6162d842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7898832684824902\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.77      0.92      0.84       303\n",
      "    Politics       0.84      0.61      0.70       211\n",
      "\n",
      "    accuracy                           0.79       514\n",
      "   macro avg       0.80      0.76      0.77       514\n",
      "weighted avg       0.80      0.79      0.78       514\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"vectorizer\", embeddingvectorizer.EmbeddingCountVectorizer(word_vectors2_dict, operator='mean')),\n",
    "        (\"classifier\", SVC(gamma=\"scale\", C=1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train10 = np.array(list(vectorizer.fit_transform(X_train)))\n",
    "X_test10 = np.array(list(vectorizer.transform(X_test)))\n",
    "classifier.fit(X_train10, y_train)\n",
    "y_pred = classifier.predict(X_test10)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8531b9",
   "metadata": {},
   "source": [
    "# Loading the unlabeled dataset and cleaning it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1fbe8",
   "metadata": {},
   "source": [
    "Now, I load the unlabeled data and clean it as I did for the labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24b82d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>blurb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nuoto, Dall?Aglio morto in palestra</td>\n",
       "      <td>Nuoto la tragedia Nuoto, Dall’Aglio morto in p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>È ufficiale, Valentino Rossi rinnova con la Ya...</td>\n",
       "      <td>\"Il pesarese ha firmato con la Casa di Iwata u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Real Madrid-Juve: minacce a moglie dell'arbitr...</td>\n",
       "      <td>\"Gli insulti sul profilo Twitter della consort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Salvini su Balotelli: \"\"Capitano della Nazion...</td>\n",
       "      <td>\"Il ministro dell'Interno e vicepremier: \"\"Spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Juve, Tardelli e la Coppa vinta all'Heysel: \"...</td>\n",
       "      <td>\"L'ex bianconero, in campo nel 1985 contro il ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                Nuoto, Dall?Aglio morto in palestra   \n",
       "1  È ufficiale, Valentino Rossi rinnova con la Ya...   \n",
       "2  Real Madrid-Juve: minacce a moglie dell'arbitr...   \n",
       "3  \"Salvini su Balotelli: \"\"Capitano della Nazion...   \n",
       "4  \"Juve, Tardelli e la Coppa vinta all'Heysel: \"...   \n",
       "\n",
       "                                               blurb  \n",
       "0  Nuoto la tragedia Nuoto, Dall’Aglio morto in p...  \n",
       "1  \"Il pesarese ha firmato con la Casa di Iwata u...  \n",
       "2  \"Gli insulti sul profilo Twitter della consort...  \n",
       "3  \"Il ministro dell'Interno e vicepremier: \"\"Spe...  \n",
       "4  \"L'ex bianconero, in campo nel 1985 contro il ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled = pd.read_csv(\"unlabeled.csv\")\n",
    "unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3685ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.title.str.contains(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad2408cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.blurb.str.contains(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "271940dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.title.str.contains(r\"</?\\w[^>]*>\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a16d5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.title.str.contains(r\"&[^;]+;\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c7c67a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.blurb.str.contains(r\"</?\\w[^>]*>\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd46761f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.blurb.str.contains(r\"&[^;]+;\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "331533f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled['title'] = unlabeled['title'].str.replace(r\"&[^;]+;\", '', regex=True)\n",
    "unlabeled['title'] = unlabeled['title'].str.replace(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\", '', regex=True)\n",
    "unlabeled['blurb'] = unlabeled['blurb'].str.replace(r\"&[^;]+;\", '', regex=True)\n",
    "unlabeled['blurb'] = unlabeled['blurb'].str.replace(r\"</?\\w[^>]*>\", '', regex=True)\n",
    "unlabeled['blurb'] = unlabeled['blurb'].str.replace(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\", '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5332ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.blurb.str.contains(r\"&[^;]+;\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f539f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.blurb.str.contains(r\"</?\\w[^>]*>\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b478259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.title.str.contains(r\"&[^;]+;\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a487e327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.blurb.str.contains(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2cd690a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.title.str.contains(r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8873d995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31575493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled['title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4caa0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled = unlabeled.dropna(subset=['blurb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56310e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled = unlabeled.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc577e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4b3f753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nuoto, Dall?Aglio morto in palestra Nuoto la t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>È ufficiale, Valentino Rossi rinnova con la Ya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Real Madrid-Juve: minacce a moglie dell'arbitr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Salvini su Balotelli: \"\"Capitano della Nazion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Juve, Tardelli e la Coppa vinta all'Heysel: \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Nuoto, Dall?Aglio morto in palestra Nuoto la t...\n",
       "1  È ufficiale, Valentino Rossi rinnova con la Ya...\n",
       "2  Real Madrid-Juve: minacce a moglie dell'arbitr...\n",
       "3  \"Salvini su Balotelli: \"\"Capitano della Nazion...\n",
       "4  \"Juve, Tardelli e la Coppa vinta all'Heysel: \"..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled['text'] = unlabeled['title'] + ' ' + unlabeled['blurb']\n",
    "new_unlabeled = unlabeled.drop(columns=[\"title\", \"blurb\"])\n",
    "new_unlabeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddede214",
   "metadata": {},
   "source": [
    "labels = {1: \"political\", 0: \"Other\"}\n",
    "data[\"topic\"] = data[\"topic\"].replace(labels)\n",
    "predictions = final_pipeline.predict(unlabeled[\"text\"])\n",
    "unlabeled[\"predictions\"] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dacd43a",
   "metadata": {},
   "source": [
    "# Fitting the models to the unlabeled data and using it to predict the topics of the articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3035b",
   "metadata": {},
   "source": [
    "Now, I create the labels for the topic of the unlabeled dataset, I then use the model I chose as the best one to predict the topic for the unlabeled datatset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28e69ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline.fit(data[\"text\"], data[\"topic\"])\n",
    "labels = {1: \"political\", 0: \"Other\"}\n",
    "data[\"topic\"] = data[\"topic\"].replace(labels)\n",
    "\n",
    "predictions = final_pipeline.predict(new_unlabeled[\"text\"])\n",
    "\n",
    "new_unlabeled[\"predictions\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d47e35cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nuoto, Dall?Aglio morto in palestra Nuoto la t...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>È ufficiale, Valentino Rossi rinnova con la Ya...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Real Madrid-Juve: minacce a moglie dell'arbitr...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Salvini su Balotelli: \"\"Capitano della Nazion...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Juve, Tardelli e la Coppa vinta all'Heysel: \"...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text predictions\n",
       "0  Nuoto, Dall?Aglio morto in palestra Nuoto la t...       Other\n",
       "1  È ufficiale, Valentino Rossi rinnova con la Ya...       Other\n",
       "2  Real Madrid-Juve: minacce a moglie dell'arbitr...       Other\n",
       "3  \"Salvini su Balotelli: \"\"Capitano della Nazion...    Politics\n",
       "4  \"Juve, Tardelli e la Coppa vinta all'Heysel: \"...       Other"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_unlabeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238210d",
   "metadata": {},
   "source": [
    "# Finally, saving the new dataset with the texts and the predicted topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33fd38a",
   "metadata": {},
   "source": [
    "First, I convert the NumPy ndarray object into a dataframe and then I save it a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a07f3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame(new_unlabeled)\n",
    "df_predictions.to_csv(\"predicted_unlabeleddata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946b997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
